# main_06_fixed.py: íŒŒì¼ ë³€ê²½ ì‹œ RAG ì²´ì¸ì´ ì˜¬ë°”ë¥´ê²Œ ì—…ë°ì´íŠ¸ë˜ë„ë¡ ìˆ˜ì •í•œ ì½”ë“œ
import streamlit as st
# from dotenv import load_dotenv
import os
import tempfile

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
# load_dotenv()

# --- 1. RAG ì‹œìŠ¤í…œì„ ìœ„í•œ í•¨ìˆ˜ ì •ì˜ (ê¸°ì¡´ê³¼ ë™ì¼) ---
@st.cache_data(show_spinner="PDF íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤...")
def process_pdf(_uploaded_file):
    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
        tmp_file.write(_uploaded_file.getvalue())
        tmp_file_path = tmp_file.name
    
    loader = PyPDFLoader(tmp_file_path)
    documents = loader.load()
    
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=100)
    split_documents = text_splitter.split_documents(documents)
    
    os.remove(tmp_file_path)
    return split_documents

@st.cache_resource(show_spinner="ë¬¸ì„œ ì„ë² ë”© ë° Retrieverë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...")
def get_retriever(_split_documents):
    embeddings_model = OpenAIEmbeddings()
    db = Chroma.from_documents(_split_documents, embeddings_model)
    return db.as_retriever()

# --- 2. Streamlit UI ì„¤ì • ë° ìƒíƒœ ì´ˆê¸°í™” ---
st.set_page_config(page_title="Updatable RAG Chatbot", page_icon="ğŸ”„")
st.title("ğŸ”„ PDF ë³€ê²½ì´ ê°€ëŠ¥í•œ RAG ì±—ë´‡")
st.write("PDFë¥¼ ì—…ë¡œë“œí•˜ê³ , ë‹¤ë¥¸ íŒŒì¼ë¡œ êµì²´í•˜ë©° ì±„íŒ…ì„ ê³„ì†í•´ë³´ì„¸ìš”!")

if "messages" not in st.session_state:
    st.session_state.messages = []
if "current_file_name" not in st.session_state:
    st.session_state.current_file_name = None

uploaded_file = st.sidebar.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type="pdf")

if uploaded_file is not None:
    if uploaded_file.name != st.session_state.current_file_name:
        st.info(f"'{uploaded_file.name}' íŒŒì¼ì— ëŒ€í•œ ì±„íŒ…ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì´ì „ ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë©ë‹ˆë‹¤.")
        st.session_state.messages = []
        st.session_state.current_file_name = uploaded_file.name

    # --- 3. RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë° ì²´ì¸ êµ¬ì„± ---
    
    processed_docs = process_pdf(uploaded_file)
    retriever = get_retriever(processed_docs)
    
    # [í•µì‹¬ ìˆ˜ì •] RAG ì²´ì¸ì„ ì„¸ì…˜ì— ì €ì¥í•˜ì§€ ì•Šê³ , íŒŒì¼ì´ ìˆì„ ë•Œë§ˆë‹¤ ìƒˆë¡œ êµ¬ì„±í•©ë‹ˆë‹¤.
    # ì´ë ‡ê²Œ í•˜ë©´ retrieverê°€ ë³€ê²½ë  ë•Œë§ˆë‹¤ ì²´ì¸ë„ ìµœì‹  ìƒíƒœë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)
    prompt_template = ChatPromptTemplate.from_template(
        """
        ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë§Œì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”. ì»¨í…ìŠ¤íŠ¸ì— ì—†ëŠ” ì •ë³´ëŠ” ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.
        [Context]
        {context}
        [Question]
        {question}
        """
    )
    output_parser = StrOutputParser()
    
    rag_chain = (
        {"context": retriever | (lambda docs: "\n\n".join(d.page_content for d in docs)), 
         "question": RunnablePassthrough()}
        | prompt_template
        | llm
        | output_parser
    )
    
    # --- 4. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„ ---
    
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("PDF ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            response = rag_chain.invoke(prompt)
            st.session_state.messages.append({"role": "assistant", "content": response})
            with st.chat_message("assistant"):
                st.markdown(response)
else:
    st.info("ì‚¬ì´ë“œë°”ì—ì„œ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ì±—ë´‡ì„ ì‹œì‘í•˜ì„¸ìš”.")
    st.session_state.messages = []
    st.session_state.current_file_name = None

